{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10516353,"sourceType":"datasetVersion","datasetId":6509402},{"sourceId":10516496,"sourceType":"datasetVersion","datasetId":6509498},{"sourceId":10517906,"sourceType":"datasetVersion","datasetId":6510319}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain-community langchain langchain-openai vllm chromadb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_community.llms import VLLM\n\nllm = VLLM(model=\"bigscience/bloom-3b\", temperature=0.7, max_new_tokens=128, top_p=0.96, top_k=20, trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:38:38.443210Z","iopub.execute_input":"2025-01-19T18:38:38.443515Z","iopub.status.idle":"2025-01-19T18:40:15.501051Z","shell.execute_reply.started":"2025-01-19T18:38:38.443488Z","shell.execute_reply":"2025-01-19T18:40:15.500156Z"}},"outputs":[{"name":"stdout","text":"WARNING 01-19 18:38:42 cuda.py:32] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\nINFO 01-19 18:38:46 config.py:2272] Downcasting torch.float32 to torch.float16.\nWARNING 01-19 18:38:46 config.py:2339] The model's config.json does not contain any of the following keys to determine the original maximum length of the model: ['max_position_embeddings', 'n_positions', 'max_seq_len', 'seq_length', 'model_max_length', 'max_sequence_length', 'max_seq_length', 'seq_len']. Assuming the model's maximum length is 2048.\nINFO 01-19 18:38:56 config.py:510] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\nINFO 01-19 18:38:56 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='bigscience/bloom-3b', speculative_config=None, tokenizer='bigscience/bloom-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=bigscience/bloom-3b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \nINFO 01-19 18:38:59 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 01-19 18:38:59 selector.py:129] Using XFormers backend.\nINFO 01-19 18:38:59 importing.py:15] Triton not installed or not compatible; certain GPU-related functions will not be available.\nINFO 01-19 18:39:10 model_runner.py:1094] Starting to load model bigscience/bloom-3b...\nINFO 01-19 18:39:10 weight_utils.py:251] Using model weights format ['*.safetensors']\nINFO 01-19 18:39:10 weight_utils.py:296] No model.safetensors.index.json found in remote.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93b3e0442c2a4e47b0841efb63235889"}},"metadata":{}},{"name":"stdout","text":"INFO 01-19 18:39:35 model_runner.py:1099] Loading model weights took 5.6083 GB\nINFO 01-19 18:39:37 worker.py:241] Memory profiling takes 2.17 seconds\nINFO 01-19 18:39:37 worker.py:241] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\nINFO 01-19 18:39:37 worker.py:241] model weights take 5.61GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 2.29GiB; the rest of the memory reserved for KV Cache is 5.28GiB.\nINFO 01-19 18:39:37 gpu_executor.py:76] # GPU blocks: 1153, # CPU blocks: 873\nINFO 01-19 18:39:37 gpu_executor.py:80] Maximum concurrency for 2048 tokens per request: 9.01x\nINFO 01-19 18:39:43 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:31<00:00,  1.10it/s]","output_type":"stream"},{"name":"stdout","text":"INFO 01-19 18:40:15 model_runner.py:1535] Graph capturing finished in 32 secs, took 0.23 GiB\nINFO 01-19 18:40:15 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 40.22 seconds\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(llm.invoke(\"Who wrote Rhapsody in Blue?\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:40:24.869085Z","iopub.execute_input":"2025-01-19T18:40:24.869390Z","iopub.status.idle":"2025-01-19T18:40:29.613510Z","shell.execute_reply.started":"2025-01-19T18:40:24.869368Z","shell.execute_reply":"2025-01-19T18:40:29.612871Z"}},"outputs":[{"name":"stderr","text":"Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.72s/it, est. speed input: 1.69 toks/s, output: 27.10 toks/s]","output_type":"stream"},{"name":"stdout","text":"\"\n\n\"No, I did not; I am sure I did not.\"\n\n\"Who wrote it?\"\n\n\"I do not know.\"\n\n\"Why do you say you do not know?\"\n\n\"I do not know.\"\n\n\"How do you know?\"\n\n\"I do not know.\"\n\n\"Why do you say you do not know?\"\n\n\"I do not know.\"\n\n\"How do you know?\"\n\n\"I do not know.\"\n\n\"Why do you say you do not know?\"\n\n\"I do not know.\"\n\n\"How do you know?\"\n\n\"I do\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import OpenAIEmbeddings\nimport chromadb\nfrom uuid import uuid4\nfrom langchain.llms import OpenAI","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:40:53.362518Z","iopub.execute_input":"2025-01-19T18:40:53.362930Z","iopub.status.idle":"2025-01-19T18:40:54.338460Z","shell.execute_reply.started":"2025-01-19T18:40:53.362901Z","shell.execute_reply":"2025-01-19T18:40:54.337740Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"folder_path = \"/kaggle/input/book-dataset/book.txt\"\nloader = TextLoader(folder_path)\ndocuments = loader.load()\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=800,\n    chunk_overlap=200\n)\n\nsplit_docs = splitter.split_documents(documents)\nembeddings = SentenceTransformerEmbeddings(model_name=\"static-retrieval-mrl-en-v1\")\ncollection = Chroma.from_documents(split_docs, embeddings, persist_directory='chroma_db')\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"Answer the question based on the book 'Perspectives in Logic by Stephen Cook and Phuong Nguyen'. \"\n        \"Context: {context}\"\n        \"Question: {input}\"),\n        (\"human\", \"{input}\"),\n    ]\n)\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\npipeline = create_retrieval_chain(collection.as_retriever(), question_answer_chain)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:41:55.750795Z","iopub.execute_input":"2025-01-19T18:41:55.751142Z","iopub.status.idle":"2025-01-19T18:42:00.913672Z","shell.execute_reply.started":"2025-01-19T18:41:55.751116Z","shell.execute_reply":"2025-01-19T18:42:00.912955Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"resp = pipeline.invoke({\"input\" : \"Give definition of Two-Sorted First-Order Vocabularies\"})\nprint(resp['answer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:47:10.163327Z","iopub.execute_input":"2025-01-19T18:47:10.163679Z","iopub.status.idle":"2025-01-19T18:47:15.680877Z","shell.execute_reply.started":"2025-01-19T18:47:10.163650Z","shell.execute_reply":"2025-01-19T18:47:15.679849Z"}},"outputs":[{"name":"stderr","text":"Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.49s/it, est. speed input: 159.00 toks/s, output: 23.31 toks/s]","output_type":"stream"},{"name":"stdout","text":"\nIV.2. Two-Sorted First-Order Logic\nIV.2.1. Syntax. Our two-sorted ﬁrst-order logic is an extension of the\n(single-sorted) ﬁrst-order logic introduced in Chapter II. Here there are\ntwo kinds of variables: the variables x, y, z, . . . of the ﬁrst sort are called\nnumber variables, and are intended to range over the natural numbers; and\nthe variables X, Y, Z, . . . of the second sort are called set (or also string)\nvariables, and are intended to range over ﬁnite subsets of natural numbers\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11}]}